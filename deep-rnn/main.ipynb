{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import traceback\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "import random as  rnd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input\n",
    "rnd.seed(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'data/'\n",
    "filename = 'shakespeare_data.txt'\n",
    "lines = []\n",
    "counter = 0\n",
    "\n",
    "with open(os.path.join(dirname, filename)) as files:\n",
    "    for line in files:        \n",
    "        pure_line = line.strip()\n",
    "        if pure_line:\n",
    "            lines.append(pure_line)\n",
    "            \n",
    "n_lines = len(lines)\n",
    "print(f\"Number of lines: {n_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(lines[506:514]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\\n\".join(lines)\n",
    "vocab = sorted(set(text))\n",
    "vocab.insert(0,\"[UNK]\")\n",
    "vocab.insert(1,\"\")\n",
    "\n",
    "print(f'{len(vocab)} unique characters')\n",
    "print(\" \".join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"Hello world!\"\n",
    "chars = tf.strings.unicode_split(line, input_encoding='UTF-8')\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab.index('a'))\n",
    "print(vocab.index('u'))\n",
    "print(vocab.index(' '))\n",
    "print(vocab.index('2'))\n",
    "print(vocab.index('3'))\n",
    "ids = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)(chars)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_tensor(line, vocab):\n",
    "    \"\"\"\n",
    "    Converts a line of text into a tensor of integer values representing characters.\n",
    "\n",
    "    Args:\n",
    "        line (str): A single line of text.\n",
    "        vocab (list): A list containing the vocabulary of unique characters.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor(dtype=int64): A tensor containing integers (unicode values) corresponding to the characters in the `line`.\n",
    "    \"\"\"\n",
    "    chars = tf.strings.unicode_split(line, input_encoding='UTF-8')\n",
    "    ids = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)(chars)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids, vocab):\n",
    "    \"\"\"\n",
    "    Converts a tensor of integer values into human-readable text.\n",
    "\n",
    "    Args:\n",
    "        ids (tf.Tensor): A tensor containing integer values (unicode IDs).\n",
    "        vocab (list): A list containing the vocabulary of unique characters.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the characters in human-readable format.\n",
    "    \"\"\"\n",
    "    chars_from_ids = tf.keras.layers.StringLookup(vocabulary=vocab, invert=True, mask_token=None)\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_from_ids(ids, vocab).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = lines[:-1000]\n",
    "eval_lines = lines[-1000:]\n",
    "\n",
    "print(f\"Number of training lines: {len(train_lines)}\")\n",
    "print(f\"Number of validation lines: {len(eval_lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = line_to_tensor(\"\\n\".join([\"Hello world!\", \"Generative AI\"]), vocab)\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "print([text_from_ids([ids], vocab).numpy() for ids in ids_dataset.take(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 10\n",
    "data_generator = ids_dataset.batch(seq_length + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in data_generator.take(2):\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for seq in data_generator.take(2):\n",
    "    print(f\"{i}. {text_from_ids(seq, vocab).numpy()}\")\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    \"\"\"\n",
    "    Splits the input sequence into two sequences, where one is shifted by one position.\n",
    "\n",
    "    Args:\n",
    "        sequence (tf.Tensor or list): A list of characters or a tensor.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor, tf.Tensor: Two tensors representing the input and output sequences for the model.\n",
    "    \"\"\"\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_dataset(lines, vocab, seq_length=100, batch_size=64):\n",
    "    \"\"\"\n",
    "    Creates a batch dataset from a list of text lines.\n",
    "\n",
    "    Args:\n",
    "        lines (list): A list of strings with the input data, one line per row.\n",
    "        vocab (list): A list containing the vocabulary.\n",
    "        seq_length (int): The desired length of each sample.\n",
    "        batch_size (int): The batch size.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: A batch dataset generator.\n",
    "    \"\"\"\n",
    "    BUFFER_SIZE = 10000\n",
    "    single_line_data  = \"\\n\".join(lines)\n",
    "    all_ids = line_to_tensor(single_line_data, vocab)\n",
    "    ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "    data_generator = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "    dataset_xy = data_generator.map(lambda x: split_input_target(x))\n",
    "    dataset = (                                   \n",
    "        dataset_xy                                \n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(batch_size, drop_remainder=True)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)  \n",
    "        )\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "dataset = create_batch_dataset(train_lines[1:100], vocab, seq_length=16, batch_size=2)\n",
    "\n",
    "print(\"Prints the elements into a single batch. The batch contains 2 elements: \")\n",
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"\\n\\033[94mInput0\\t:\", text_from_ids(input_example[0], vocab).numpy())\n",
    "    print(\"\\n\\033[93mTarget0\\t:\", text_from_ids(target_example[0], vocab).numpy())\n",
    "    \n",
    "    print(\"\\n\\n\\033[94mInput1\\t:\", text_from_ids(input_example[1], vocab).numpy())\n",
    "    print(\"\\n\\033[93mTarget1\\t:\", text_from_ids(target_example[1], vocab).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = create_batch_dataset(train_lines, vocab, seq_length=100, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULM(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    A GRU-based language model that maps from a tensor of tokens to activations over a vocabulary.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int, optional): Size of the vocabulary. Defaults to 256.\n",
    "        embedding_dim (int, optional): Depth of embedding. Defaults to 256.\n",
    "        rnn_units (int, optional): Number of units in the GRU cell. Defaults to 128.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A GRULM language model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=256, embedding_dim=256, rnn_units=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size, activation='log_softmax')\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        x, states = self.gru(x, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "        return x, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 82\n",
    "embedding_dim = 256\n",
    "rnn_units = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    input_layer = tf.keras.Input(shape=(None,), batch_size=BATCH_SIZE)\n",
    "    model = GRULM(vocab_size=vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units)\n",
    "    model.call(input_layer)\n",
    "    model.summary()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # print(\"\\033[91mError! \\033[0mA problem occurred while building your model. This error can occur due to wrong initialization of the return_sequences parameter\\n\\n\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(\"Input: \", input_example_batch[0].numpy()) # Lets use only the first sequence on the batch\n",
    "    example_batch_predictions, _ = model(tf.constant([input_example_batch[0].numpy()]))\n",
    "    print(\"\\n\",example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.math.argmax(example_batch_predictions[0], axis=1)\n",
    "print(sampled_indices.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0], vocab))\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    \"\"\"\n",
    "    Sets the loss and optimizer for the given model\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The model to compile.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The compiled model.\n",
    "    \"\"\"\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.00125)\n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "model = compile_model(model)\n",
    "history = model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# # Define the output directory and file path\n",
    "# output_dir = './model/'\n",
    "# output_file = os.path.join(output_dir, 'model.weights.h5')\n",
    "\n",
    "# # Remove the directory if it exists\n",
    "# try:\n",
    "#     shutil.rmtree(output_dir)\n",
    "# except OSError as e:\n",
    "#     pass\n",
    "\n",
    "# # Create the directory\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Save model weights to the specified file\n",
    "# model.save_weights(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_perplexity(preds, target):\n",
    "    \"\"\"\n",
    "    Function to calculate the log perplexity of a model.\n",
    "\n",
    "    Args:\n",
    "        preds (tf.Tensor): Predictions from the model.\n",
    "        target (tf.Tensor): True target values.\n",
    "\n",
    "    Returns:\n",
    "        float: The log perplexity of the model.\n",
    "    \"\"\"\n",
    "    PADDING_ID = 1\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    log_p = tf.reduce_sum(preds * tf.one_hot(target, depth=preds.shape[-1]), axis=-1)\n",
    "    non_pad = 1.0 - tf.cast(tf.equal(target, PADDING_ID), dtype=log_p.dtype)\n",
    "    log_p = log_p * non_pad\n",
    "\n",
    "    log_p_sum = tf.reduce_sum(log_p, axis=-1)\n",
    "    non_pad_sum = tf.reduce_sum(non_pad, axis=-1)\n",
    "\n",
    "    return -log_p_sum / non_pad_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "\n",
    "model = GRULM(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units = rnn_units)\n",
    "model.build(input_shape=(100, vocab_size))\n",
    "model.load_weights('./model/model.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_text = \"\\n\".join(eval_lines)\n",
    "eval_ids = line_to_tensor([eval_text], vocab)\n",
    "input_ids, target_ids = split_input_target(tf.squeeze(eval_ids, axis=0))\n",
    "\n",
    "preds = model(tf.expand_dims(input_ids, 0), training=False)\n",
    "print(type(preds), len(preds), preds[0].shape if isinstance(preds, tuple) else preds.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ppx = log_perplexity(preds, tf.expand_dims(target_ids, 0))\n",
    "print(f'The log perplexity and perplexity of your model are {log_ppx} and {np.exp(log_ppx)} respectively')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_random_sampling(log_probs, temperature=1.0):\n",
    "    \"\"\"Temperature Random sampling from a categorical distribution. The higher the temperature, the more \n",
    "       random the output. If temperature is close to 0, it means that the model will just return the index\n",
    "       of the character with the highest input log_score\n",
    "    \n",
    "    Args:\n",
    "        log_probs (tf.Tensor): The log scores for each characeter in the dictionary\n",
    "        temperature (number): A value to weight the random noise. \n",
    "    Returns:\n",
    "        int: The index of the selected character\n",
    "    \"\"\"\n",
    "    u = tf.random.uniform(minval=1e-6, maxval=1.0 - 1e-6, shape=log_probs.shape)\n",
    "    g = -tf.math.log(-tf.math.log(u))\n",
    "    return tf.math.argmax(log_probs + g * temperature, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeModel:\n",
    "    def __init__(self, model, vocab, temperature=1.0):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.temperature = temperature\n",
    "        self.idx_to_char = tf.keras.layers.StringLookup(\n",
    "            vocabulary=vocab, invert=True)\n",
    "        self.char_to_idx = tf.keras.layers.StringLookup(\n",
    "            vocabulary=vocab)\n",
    "\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        inputs = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        inputs = self.char_to_idx(inputs)\n",
    "        inputs = tf.expand_dims(inputs, 0)\n",
    "        \n",
    "        print(f\"Inputs shape: {inputs.shape}\")  # Debugging print\n",
    "        \n",
    "        preds = self.model(inputs, training=False)\n",
    "\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "\n",
    "        preds = preds / self.temperature\n",
    "\n",
    "        predicted_id = tf.random.categorical(preds[0], num_samples=1)[-1, 0].numpy()\n",
    "        next_char = self.idx_to_char(predicted_id)\n",
    "        return next_char\n",
    "\n",
    "    def generate_n_chars(self, num_chars, start_string):\n",
    "        result = [start_string]\n",
    "        next_char = start_string\n",
    "        for _ in range(num_chars):\n",
    "            next_char = self.generate_one_step(next_char)\n",
    "            result.append(next_char)\n",
    "\n",
    "        return tf.strings.join(result)[0].numpy().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(272)\n",
    "gen = GenerativeModel(model, vocab, temperature=0.5)\n",
    "\n",
    "print(gen.generate_n_chars(32, \" \"), '\\n\\n' + '_'*80)\n",
    "print(gen.generate_n_chars(32, \"Dear\"), '\\n\\n' + '_'*80)\n",
    "print(gen.generate_n_chars(32, \"KING\"), '\\n\\n' + '_'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(np.random.randint(1, 1000))\n",
    "gen = GenerativeModel(model, vocab, temperature=0.8)\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "print(gen.generate_n_chars(1000, \"ROMEO \"), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', time.time() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
